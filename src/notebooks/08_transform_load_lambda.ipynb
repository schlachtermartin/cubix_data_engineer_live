from io import StringIO
import json
import boto3
import pandas as pd

def taxi_trips_transformations(taxi_trips: pd.DataFrame) -> pd.DataFrame:
    """Perform transformation with the taxi data.

    Args:
        taxi_trips (pd.DataFrame): 
            - The DataFrame holding the daily taxi trips

    Returns:
        pd.DataFrame: 
            - The cleaned, transformed DataFrame holding the daily taxi trips.
    """

    if not isinstance(taxi_trips, pd.DataFrame):
        raise TypeError("taxi_trips is not a valid pandas DataFrame.")

    taxi_trips.drop(["pickup_census_tract", "dropoff_census_tract", 
                     "pickup_centroid_location", "dropoff_centroid_location"], axis=1, inplace=True)

    taxi_trips.dropna(inplace=True)

    taxi_trips.rename(columns={"pickup_community_area": "pickup_community_area_id",
                            "dropoff_community_area": "dropoff_community_area_id"}, inplace=True)

    taxi_trips["datetime_for_weather"] = pd.to_datetime(taxi_trips["trip_start_timestamp"]).dt.floor("h")

    return taxi_trips
    
def update_taxi_trips_with_master_data(taxi_trips: pd.DataFrame, payment_type_master: pd.DataFrame, company_master: pd.DataFrame) -> pd.DataFrame:
    """Update the tayi_trips DataFrame with the company_master and payment_type_master ids, and delete the string columns.
    Args:
        taxi_trips (pd.DataFrame):
            - The DataFrame with the daily taxi trips.
        payment_type_master (pd.DataFrame):
            - The payment type master table.
        company_master (pd.DataFrame):
            - The company master table.

    Returns:
        pd.DataFrame:
            - The taxi trips data, with only payment_type_id and company_id, without company or payment_type values.
    """


    taxi_trips_id = taxi_trips.merge(payment_type_master, on="payment_type")
    taxi_trips_id = taxi_trips_id.merge(company_master, on="company")

    taxi_trips_id.drop(["payment_type", "company"], axis=1, inplace=True)

    return taxi_trips_id

def update_master(taxi_trips: pd.DataFrame, master: pd.DataFrame, id_columns: str, value_column: str) -> pd.DataFrame:
    """Extend the master DataFrame with new values if there are any.

    Args:
        taxi_trips (pd.DataFrame): 
            - DateFrame holding the daily taxi trips.
        master (pd.DataFrame): 
            - DateFrame holding the master data.
        id_columns (str):
            - The id column of the master DataFrame.
        value_column (str):
            - Name of the column on master_df containing the values.
    Returns:
        pd.DataFrame: 
            - The updated master data, if new values are in the taxi data, they will be loaded to it.
    """

    max_id = master[id_columns].max()
    
    new_values_list = list(set(taxi_trips[value_column].values) - set(master[value_column].values))
    new_values_df = pd.DataFrame({
        id_columns: range(max_id + 1, max_id + len(new_values_list) + 1),
        value_column: new_values_list
    })

    updated_master = pd.concat([master, new_values_df], ignore_index=True)

    return updated_master

def transform_weather_data(weather_data: json) -> pd.DataFrame:
    """Make transformations on the daily weather api response.

    Args:
        weather_data (json):
            - The daily weather data from the Open Meteo API.

    Returns:
        pd.DataFrame:
            - A DataFrame representation of the data.
    """

    weather_data_filtered = {
        "datetime":weather_data['hourly']['time'],
        "tempretaure":weather_data['hourly']['temperature_2m'],
        "wind_speed":weather_data['hourly']['wind_speed_10m'],
        "rain":weather_data['hourly']['rain'],
        "precipitation":weather_data['hourly']['precipitation']
    }

    weather_df = pd.DataFrame(weather_data_filtered)

    weather_df["datetime"] = pd.to_datetime(weather_df["datetime"])

    return weather_df

def read_json_from_s3(bucket: str, path: str) -> json:
    """Reads a JSON file from an S3 bucket and returns its content as a Python object.
    
    Args:
        bucket (str):
            - The name of the S3 bucket where the JSON file is stored.
        path (str):
            - The path (key) to the JSON file within the S3 bucket.

    Returns:
        json:
            - The content of the JSON file, parsed into a Python object.
    """
    s3 = boto3.client("s3")
    
    response = s3.get_object(Bucket=bucket, Key=path)
    content = response["Body"]
    data_json = json.loads(content.read())
    
    return data_json
    
def read_csv_from_s3(bucket: str, path: str, filename: str) -> pd.DataFrame:
    """Downloads a csv file from an S3 bucket
    
    Args:
        bucket (str):
            - The bucket where the files at.
        path (str):
            - The folders to the file.
        filename (str):
            - Name of the file.

    Returns:
        pd.DataFrame:
            - A DataFrame of the downloaded file.
    """
    s3 = boto3.client("s3")
    
    full_path = f"{path}{filename}"
    
    object = s3.get_object(Bucket=bucket, Key=full_path)
    object = object["Body"].read().decode("utf-8")
    output_df = pd.read_csv(StringIO(object))
    
    return output_df

def upload_dateframe_to_s3(dataframe: pd.DataFrame, bucket: str, path: str):
    """Uploads a dataframe to the specified s3 path.
    
    Args:
        dataframe (pd.DateFrame):
            - The dataframe to be uploaded.
        bucket (str):
            - Name of the S3 bucket where we want to store the files.
        path (str):
            - Path within the bucket to upload the files.
        
    Returns:
        None
    """
    s3 = boto3.client("s3")
    buffer = StringIO()
    dataframe.to_csv(buffer, index=False)
    df_content = buffer.getvalue()
    s3.put_object(Bucket=bucket, Key=path, Body=df_content)


def upload_master_data_to_s3(bucket: str, path: str, file_type: str, dataframe: pd.DataFrame):
    """Uploads master data (payment_type or company) to S3. Copies the previous version and creates the new one.
    
    Args:
        bucket (str):
            - Name of the S3 bucket where we want to store the files.
        path (str):
            - Path within the bucket to upload the files.
        file_type (str):
            - Either "company" or "payment_type"
        dataframe (pd.DateFrame):
            - The dataframe to be uploaded.

    Returns:
        None
    """
    
    
    s3 = boto3.client("s3")
    
    master_file_path = f"{path}{file_type}_master.csv"
    previous_master_file_path = f"transformed_data/master_data_previous_version/{file_type}_master_previous_version.csv"
    
    s3.copy_object(
        Bucket=bucket,
        CopySource={"Bucket": bucket, "Key": master_file_path},
        Key=previous_master_file_path
    )
    
    upload_dateframe_to_s3(bucket= bucket, dataframe= dataframe, path=master_file_path)

def upload_and_move_file_on_s3(
        dataframe: pd.DataFrame, 
        datetime_col: str, 
        bucket: str,
        file_type: str,
        filename: str,
        source_path: str, 
        target_path_raw: str, 
        target_path_transformed: str
    ):
    """Uploads a DataFrame to S3 and then moves a file from the base folder to another.
    
    Args:
        bucket (str):
            - Name of the S3 bucket.
        file_type (str):
            - "weather" or "taxi".
        source_path (str):
            - Source path within the bucket.
        target_path_transformed (str):
            - Target path within the bucket where the transformed data would go.
        target_path_raw (str):
            - Target path within the bucket where the raw data would go.
        filename (str, optinal):
            - Name of the file to be uploaded or moved.
        dataframe (pd.DateFrame):
            - The dataframe to be uploaded.
        datetime_col (str, optinal)
            - Datetime column name, which we derive the date for the filename.

    Returns:
        None
    """
    s3 = boto3.client("s3")
    
    formatted_date = dataframe[datetime_col].iloc[0].strftime("%Y-%m-%d")
    new_path_with_filename = f"{target_path_transformed}{file_type}_{formatted_date}.csv"
    
    upload_dateframe_to_s3(bucket= bucket, dataframe= dataframe, path=new_path_with_filename)
    
    s3.copy_object(
        Bucket=bucket,
        CopySource={"Bucket": bucket, "Key": f"{source_path}{filename}"},
        Key=f"{target_path_raw}{filename}"
    )
    
    s3.delete_object(Bucket= bucket, Key=f"{source_path}{filename}")

#
#
# MAIN FUNCTION
#
#

def lambda_handler(event, context):
    s3 = boto3.client("s3")
    bucket = "cubix-chicago-taxi-sm"
    raw_weather_folder = "raw_data/to_processed/weather_data/"
    raw_taxi_trips_folder = "raw_data/to_processed/taxi_data/"
    target_taxi_trips_folder = "raw_data/processed/taxi_data/"
    target_weather_folder = "raw_data/processed/weather_data/"
    
    transformed_taxi_trips_folder = "transformed_data/taxi_trips/"
    transformed_weather_folder = "transformed_data/weather/"
    
    payment_type_master_folder = "transformed_data/payment_type/"
    company_master_folder = "transformed_data/company/"
    
    payment_type_master_filename = "payment_type_master.csv"
    company_master_filename = "company_master.csv"
    
    payment_type_master = read_csv_from_s3(bucket=bucket, path=payment_type_master_folder, filename=payment_type_master_filename)
    company_master = read_csv_from_s3(bucket=bucket, path=company_master_folder, filename=company_master_filename)

    
    # TAXI DATA TRANSFORMATION AND LOADING
    for file in s3.list_objects(Bucket=bucket, Prefix=raw_taxi_trips_folder)["Contents"]:
        taxi_trip_key = file["Key"]
        
        if taxi_trip_key.split("/")[-1].strip() != "":
            if taxi_trip_key.split(".")[1] == "json":
                
                filename = taxi_trip_key.split("/")[-1]
                
                taxi_trips_data_json_test = read_json_from_s3(bucket= bucket, path= taxi_trip_key)
                
                taxi_trip_data_raw = pd.DataFrame(taxi_trip_data_json)
                taxi_trips_transformed = taxi_trips_transformations(taxi_trip_data_raw)
                
                company_master_updated = update_master(taxi_trips= taxi_trips_transformed, master=company_master, id_columns="company_id", value_column="company")
                payment_type_master_updated = update_master(taxi_trips= taxi_trips_transformed, master=payment_type_master, id_columns="payment_type_id", value_column="payment_type")
    
                taxi_trips = update_taxi_trips_with_master_data(taxi_trips_transformed, payment_type_master_updated, company_master_updated)
                
                upload_and_move_file_on_s3(
                    dataframe= taxi_trips, 
                    datetime_col= "datetime_for_weather", 
                    bucket= bucket,
                    file_type= "taxi",
                    filename= filename,
                    source_path= raw_taxi_trips_folder, 
                    target_path_raw= target_taxi_trips_folder, 
                    target_path_transformed= transformed_taxi_trips_folder
                    )
                print("taxi_trips is uploaded and moved.")
                
                upload_master_data_to_s3(bucket= bucket, path= payment_type_master_folder, file_type= "payment_type", dataframe= payment_type_master_updated)
                print("payment_type_master has been updated.")
                upload_master_data_to_s3(bucket= bucket, path= company_master_folder, file_type= "company", dataframe= company_master_updated)
                print("company_master has been updated.")
                
    #WEATHER DATA TRANSFORMATION AND LOADING
    for file in s3.list_objects(Bucket=bucket, Prefix=raw_weather_folder)["Contents"]:
        weather_key = file["Key"]
        
        if weather_key.split("/")[-1].strip() != "":
            if weather_key.split(".")[1] == "json":
                
                filename = weather_key.split("/")[-1]
                
                weather_data_json = read_json_from_s3(bucket= bucket, path= weather_key)
                
                weather_data = transform_weather_data(weather_data_json)
                
                upload_and_move_file_on_s3(
                    dataframe= weather_data,
                    datetime_col= "datetime",
                    bucket= bucket,
                    file_type= "weather",
                    filename= filename,
                    source_path= raw_weather_folder,
                    target_path_raw= target_weather_folder,
                    target_path_transformed= transformed_weather_folder
                )
                print("weather is uploaded and moved.")
        
